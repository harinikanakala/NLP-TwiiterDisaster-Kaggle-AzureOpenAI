{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core import Experiment\n",
        "from azureml.core.webservice import Webservice\n",
        "from azureml.core.image import ContainerImage\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "print(azureml.core.VERSION)\n",
        "\n",
        "subscription_id=\"XXXXXXX\"\n",
        "resource_group=\"XXXXXX\"\n",
        "workspace_name=\"XXXXXX\"\n",
        "compute=\"XXXXXXXX\"\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "\n",
        "#Create New Experiment\n",
        "exp = Experiment(ws, name = \"NLPOpenAI\")\n",
        "\n",
        "import datetime\n",
        "run = exp.start_logging(snapshot_directory=None)\n",
        "run.log(\"Experimentation Start Time\",str(datetime.datetime.now()))\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.51.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1710283349645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install kaggle"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709667570519
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "# Load AzureML workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Authenticate Kaggle (if necessary)\n",
        "# You can authenticate Kaggle using your Kaggle API token or username and API key\n",
        "\n",
        "# Set Kaggle configuration (replace 'username' and 'api_key' with your actual Kaggle credentials)\n",
        "#os.environ['KAGGLE_USERNAME'] = 'XXXXXXX'\n",
        "#os.environ['KAGGLE_KEY'] = 'XXXXXXXXXX'\n",
        "\n",
        "# Download Kaggle dataset\n",
        "#!kaggle competitions download -c nlp-getting-started\n",
        "\n",
        "# Unzip downloaded dataset (if necessary)\n",
        "#!unzip nlp-getting-started.zip\n",
        "\n",
        "# Upload dataset to AzureML datastore\n",
        "ds = ws.get_default_datastore()\n",
        "#ds.upload(src_dir='./', target_path='DisasterTweet', overwrite=True, show_progress=True)\n",
        "\n",
        "# Create dataset from the uploaded files\n",
        "dataset = Dataset.File.from_files(path=(ds, 'DisasterTweet/*.csv'))\n",
        "\n",
        "# Register dataset\n",
        "#dataset = dataset.register(workspace=ws, name='DisasterTweet', description='DisasterTweet_dataset')\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283367151
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283380207
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
        "#test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
        "default_ds = ws.get_default_datastore()\n",
        "#dataset = Dataset.get_by_name(ws, name='DisasterTweet')\n",
        "train_dataset = Dataset.Tabular.from_delimited_files(path=(default_ds, 'DisasterTweet/train.csv'))\n",
        "train_df = train_dataset.to_pandas_dataframe()\n",
        "test_dataset = Dataset.Tabular.from_delimited_files(path=(default_ds, 'DisasterTweet/test.csv'))\n",
        "test_df = test_dataset.to_pandas_dataframe()\n",
        "sub_dataset = Dataset.Tabular.from_delimited_files(path=(default_ds, 'DisasterTweet/sample_submission.csv'))\n",
        "sub_df = sub_dataset.to_pandas_dataframe()\n",
        "train_df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "  id keyword location                                               text  \\\n0  1    None     None  Our Deeds are the Reason of this #earthquake M...   \n1  4    None     None             Forest fire near La Ronge Sask. Canada   \n2  5    None     None  All residents asked to 'shelter in place' are ...   \n3  6    None     None  13,000 people receive #wildfires evacuation or...   \n4  7    None     None  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0     1.0  \n1     1.0  \n2     1.0  \n3     1.0  \n4     1.0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>None</td>\n      <td>None</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>None</td>\n      <td>None</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>None</td>\n      <td>None</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>None</td>\n      <td>None</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>None</td>\n      <td>None</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283384454
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df[\"target\"] == 0][\"text\"].values[1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'I love fruits'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283386561
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df[\"target\"] == 1][\"text\"].values[1]\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "'Forest fire near La Ronge Sask. Canada'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283392759
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building vectors\n",
        "The theory behind the model is the words contained in each tweet are a good indicator of whether they're about a real disaster or not.\n",
        "\n",
        "We'll use scikit-learn's CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process.\n",
        "\n",
        "Note: a vector is, in this context, a set of numbers that a machine learning model can work with. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
        "\n",
        "## let's get counts for the first 5 tweets in the data\n",
        "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283395325
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
        "print(example_train_vectors[0].todense().shape)\n",
        "print(example_train_vectors[0].todense())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(1, 54)\n[[0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n  0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283397173
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when running for the entire dataset, its showing none object type, so we have to check for missing values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
        "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
        "# i.e. that the train and test vectors use the same set of tokens.\n",
        "test_vectors = count_vectorizer.transform(test_df[\"text\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709684296916
        },
        "editable": false,
        "run_control": {
          "frozen": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the entire DataFrame\n",
        "missing_values = train_df.isnull().sum()\n",
        "\n",
        "# Display the count of missing values for each column\n",
        "print(missing_values)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "id             0\nkeyword      404\nlocation    3311\ntext         782\ntarget      1217\ndtype: int64\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283402461
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the \"text\" column\n",
        "missing_values_text = train_df[\"text\"].isnull().sum()\n",
        "\n",
        "# Display the count of missing values in the \"text\" column\n",
        "print(\"Missing values in 'text' column:\", missing_values_text)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing values in 'text' column: 782\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710283406710
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use openAI to generate text and fill missing values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting openai==0.28.1\n  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n\u001b[K     |████████████████████████████████| 76 kB 4.8 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==0.28.1) (3.9.3)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==0.28.1) (4.65.0)\nRequirement already satisfied: requests>=2.20 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from openai==0.28.1) (2.31.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.9.2)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (23.1.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (4.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from aiohttp->openai==0.28.1) (1.3.3)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (2022.9.24)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (1.26.18)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.20->openai==0.28.1) (3.1.0)\n\u001b[31mERROR: llama-index 0.9.45.post1 has requirement openai>=1.1.0, but you'll have openai 0.28.1 which is incompatible.\u001b[0m\nInstalling collected packages: openai\n  Attempting uninstall: openai\n    Found existing installation: openai 0.28.0\n    Uninstalling openai-0.28.0:\n      Successfully uninstalled openai-0.28.0\nSuccessfully installed openai-0.28.1\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the OpenAI API client with your API key\n",
        "import os\n",
        "import openai\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"XXXXXXXXX\"\n",
        "openai.api_version = \"XXXXXXX\"\n",
        "openai.api_key = \"XXXXXXXXX\"\n",
        "deployment_name='gpt-35-turbo-model'\n",
        "\n",
        "\n",
        "\n",
        "def generate_text(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"gpt-35-turbo-model\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.3,\n",
        "        max_tokens=250,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=None\n",
        "        )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "# Example usage to generate text for missing values\n",
        "missing_text_indices = train_df[train_df['text'].isnull()].index\n",
        "import time\n",
        "total_attempts = len(missing_text_indices)\n",
        "print(\"total_attempts :\", total_attempts)\n",
        "successful_attempts = 0\n",
        "for idx in missing_text_indices:\n",
        "    prompt = \"Generate text for missing value in row \" + str(idx)\n",
        "    generated_text = generate_text(prompt)\n",
        "    if generated_text is not None:\n",
        "        train_df.at[idx, 'text'] = generated_text\n",
        "        successful_attempts += 1\n",
        "    else:\n",
        "        print(\"Failed to generate text for row\", idx)\n",
        "    time.sleep(10)  # Adjust the delay time as needed\n",
        "\n",
        "# Convert the modified DataFrame back to a Dataset\n",
        "updated_test_dataset = Dataset.Tabular.register_pandas_dataframe(train_df, (default_ds, 'DisasterTweet/updated_train.csv'), description='Updated train dataset with missing text values filled.', workspace=ws)\n",
        "\n",
        "# You can also update the existing Dataset directly if required\n",
        "# dataset.update_from_dataframe(updated_train_df)\n",
        "\n",
        "print(\"Successful attempts:\", successful_attempts)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710293260475
        },
        "editable": true,
        "run_control": {
          "frozen": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the \"text\" column\n",
        "missing_values_text = train_df[\"text\"].isnull().sum()\n",
        "\n",
        "# Display the count of missing values in the \"text\" column\n",
        "print(\"Missing values in 'text' column:\", missing_values_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing values in 'text' column: 0\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710293447763
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the \"text\" column\n",
        "missing_values_text = test_df[\"text\"].isnull().sum()\n",
        "\n",
        "# Display the count of missing values in the \"text\" column\n",
        "print(\"Missing values in 'text' column:\", missing_values_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing values in 'text' column: 354\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710293458614
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI API client with your API key\n",
        "import os\n",
        "import openai\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"XXXXXXXXX\"\n",
        "openai.api_version = \"XXXXXXX\"\n",
        "openai.api_key = \"XXXXXXXXX\"\n",
        "deployment_name='gpt-35-turbo-model'\n",
        "\n",
        "\n",
        "\n",
        "def generate_text(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"gpt-35-turbo-model\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.3,\n",
        "        max_tokens=250,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=None\n",
        "        )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "# Example usage to generate text for missing values\n",
        "missing_text_indices = test_df[test_df['text'].isnull()].index\n",
        "import time\n",
        "total_attempts = len(missing_text_indices)\n",
        "print(\"total_attempts :\", total_attempts)\n",
        "successful_attempts = 0\n",
        "for idx in missing_text_indices:\n",
        "    prompt = \"Generate text for missing value in row \" + str(idx)\n",
        "    generated_text = generate_text(prompt)\n",
        "    if generated_text is not None:\n",
        "        test_df.at[idx, 'text'] = generated_text\n",
        "        successful_attempts += 1\n",
        "    else:\n",
        "        print(\"Failed to generate text for row\", idx)\n",
        "    time.sleep(10)  # Adjust the delay time as needed\n",
        "\n",
        "# Convert the modified DataFrame back to a Dataset\n",
        "updated_test_dataset = Dataset.Tabular.register_pandas_dataframe(test_df, (default_ds, 'DisasterTweet/updated_test.csv'), description='Updated test dataset with missing text values filled.', workspace=ws)\n",
        "\n",
        "# You can also update the existing Dataset directly if required\n",
        "# dataset.update_from_dataframe(updated_train_df)\n",
        "\n",
        "print(\"Successful attempts:\", successful_attempts)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710298316651
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the \"text\" column\n",
        "missing_values_text = test_df[\"text\"].isnull().sum()\n",
        "\n",
        "# Display the count of missing values in the \"text\" column\n",
        "print(\"Missing values in 'text' column:\", missing_values_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing values in 'text' column: 0\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299412080
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
        "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
        "# i.e. that the train and test vectors use the same set of tokens.\n",
        "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299420183
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Our vectors are really big, so we want to push our model's weights\n",
        "## toward 0 without completely discounting different words - ridge regression \n",
        "## is a good way to do this.\n",
        "clf = linear_model.RidgeClassifier()"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299422070
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the \"text\" column\n",
        "missing_values_text = train_df[\"target\"].isnull().sum()\n",
        "\n",
        "# Display the count of missing values in the \"text\" column\n",
        "print(\"Missing values in 'text' column:\", missing_values_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing values in 'text' column: 1217\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299592693
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Assuming train_df is your DataFrame with missing values\n",
        "# Create an imputer object with a mean filling strategy\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit the imputer on the training data\n",
        "imputer.fit(train_df[['target']])\n",
        "\n",
        "# Transform the training and test data\n",
        "train_df['target'] = imputer.transform(train_df[['target']])\n",
        "#test_df['target'] = imputer.transform(test_df[['target']])\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/impute/_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  mode = stats.mode(array)\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299644492
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the \"text\" column\n",
        "missing_values_text = train_df[\"target\"].isnull().sum()\n",
        "\n",
        "# Display the count of missing values in the \"text\" column\n",
        "print(\"Missing values in 'text' column:\", missing_values_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Missing values in 'text' column: 0\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299661383
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
        "scores"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "array([0.57537399, 0.54574468, 0.64380531])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299666061
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_vectors, train_df[\"target\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "RidgeClassifier()"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299670925
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming clf.predict(test_vectors) returns predictions for all rows in test_vectors\n",
        "\n",
        "# Check the length of predictions\n",
        "print(len(clf.predict(test_vectors)))\n",
        "\n",
        "# Check the length of sub_df\n",
        "print(len(sub_df))\n",
        "\n",
        "# Make sure the lengths match\n",
        "\n",
        "# Ensure proper alignment of DataFrame index and predictions\n",
        "#sub_df[\"target\"] = clf.predict(test_vectors)[:len(sub_df)].tolist()\n",
        "# Ensure proper alignment of DataFrame index and predictions\n",
        "sub_df[\"target\"] = clf.predict(test_vectors)[:len(sub_df)].tolist()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3615\n3263\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299675082
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Save the trained model in the outputs folder\n",
        "import os\n",
        "print(\"Saving model...\")\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "model_file = os.path.join('outputs', 'DisasterTweets_model.pkl')\n",
        "joblib.dump(value=clf, filename=model_file)\n",
        "\n",
        "\n",
        "# Register the model\n",
        "print('Registering model...')\n",
        "Model.register(workspace=run.experiment.workspace,\n",
        "               model_path = model_file,\n",
        "               model_name = 'DisasterTweet_model')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710299684515
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import os\n",
        "# Fetches latest model\n",
        "model = ws.models['DisasterTweet_model']\n",
        "print(model.name, 'version', model.version)\n",
        "\n",
        "# Download the model file to a temporary directory\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "model.download(target_dir=temp_dir, exist_ok=True)\n",
        "\n",
        "# Get the path to the downloaded model file\n",
        "model_path = os.path.join(temp_dir, 'DisasterTweets_model.pkl')\n",
        "loaded_model = joblib.load(model_path)\n",
        "#y = loaded_model.predict([[0,148,72,35,0,33.6,0.627, 50]])\n",
        "sub_df[\"target\"] = clf.predict(test_vectors)[:len(sub_df)].tolist()\n",
        "print(\"the Output is:\", sub_df[\"target\"])"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710302423850
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.log(\"experiment End Time:\", str(datetime.datetime.now()))\n",
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": 66,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709755482345
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.get_portal_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<bound method HasRunPortal.get_portal_url of Run(Experiment: BasicNLP,\nId: 63606d77-ab5a-419d-901a-5bae1bcc4b91,\nType: None,\nStatus: NotStarted)>\n"
        }
      ],
      "execution_count": 67,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709755501763
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df[\"target\"].head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709754882953
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.to_csv(\"submission.csv\", index=False)"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709754729293
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}